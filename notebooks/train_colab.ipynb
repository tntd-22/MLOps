{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Project - Fashion MNIST Training\n",
    "\n",
    "This notebook runs 5 experiments on Google Colab with GPU acceleration and logs all metrics to DagsHub MLflow.\n",
    "\n",
    "## Experiments:\n",
    "1. **Baseline CNN** - No regularization (observe overfitting)\n",
    "2. **CNN + Regularization** - BatchNorm + Dropout (reduce overfitting)\n",
    "3. **CNN + Data Augmentation** - Simulate data enrichment (best generalization)\n",
    "4. **Hyperparameter Tuning** - Optimize learning rate, batch size, epochs\n",
    "5. **Simple MLP** - Demonstrate underfitting on image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q mlflow torch torchvision scikit-learn pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Warning: GPU not available, using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure DagsHub MLflow\n",
    "\n",
    "**Important**: Replace `YOUR_USERNAME` and `YOUR_TOKEN` with your DagsHub credentials.\n",
    "\n",
    "Get these from: https://dagshub.com/YOUR_USERNAME/MLOps → Remote → MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURE YOUR DAGSHUB CREDENTIALS HERE\n",
    "# ============================================\n",
    "DAGSHUB_USERNAME = \"YOUR_USERNAME\"  # <-- CHANGE THIS\n",
    "DAGSHUB_TOKEN = \"YOUR_TOKEN\"        # <-- CHANGE THIS\n",
    "DAGSHUB_REPO_NAME = \"MLOps\"\n",
    "\n",
    "# Set up MLflow tracking\n",
    "MLFLOW_TRACKING_URI = f\"https://dagshub.com/{DAGSHUB_USERNAME}/{DAGSHUB_REPO_NAME}.mlflow\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = DAGSHUB_USERNAME\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = DAGSHUB_TOKEN\n",
    "\n",
    "print(f\"MLflow Tracking URI: {MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for Fashion MNIST.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (1, 28, 28)\n",
    "        -> Conv2d(1, 32, 3, padding=1) -> [BatchNorm2d] -> ReLU -> MaxPool2d(2)\n",
    "        -> Conv2d(32, 64, 3, padding=1) -> [BatchNorm2d] -> ReLU -> MaxPool2d(2)\n",
    "        -> Flatten\n",
    "        -> Linear(64*7*7, 128) -> ReLU -> [Dropout]\n",
    "        -> Linear(128, 10)\n",
    "        Output (10 classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_batchnorm=False, dropout_rate=0.0):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if use_batchnorm else nn.Identity()\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if use_batchnorm else nn.Identity()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for Fashion MNIST.\n",
    "    Used to demonstrate underfitting compared to CNN.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_model(model_type=\"cnn\", use_batchnorm=False, dropout_rate=0.0):\n",
    "    if model_type.lower() == \"cnn\":\n",
    "        return CNN(use_batchnorm=use_batchnorm, dropout_rate=dropout_rate)\n",
    "    elif model_type.lower() == \"mlp\":\n",
    "        return MLP()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model architectures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_transforms(use_augmentation=False):\n",
    "    \"\"\"Get data transforms for training and validation.\"\"\"\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    if use_augmentation:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = val_transform\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "\n",
    "def get_dataloaders(batch_size=64, use_augmentation=False):\n",
    "    \"\"\"Create data loaders for Fashion MNIST.\"\"\"\n",
    "    \n",
    "    train_transform, val_transform = get_transforms(use_augmentation)\n",
    "    \n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        root=\"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = datasets.FashionMNIST(\n",
    "        root=\"./data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Download and verify dataset\n",
    "train_loader, val_loader = get_dataloaders()\n",
    "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def train_model(\n",
    "    experiment_name,\n",
    "    model_type=\"cnn\",\n",
    "    use_batchnorm=False,\n",
    "    dropout_rate=0.0,\n",
    "    use_augmentation=False,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    description=\"\",\n",
    "    save_model=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with MLflow tracking.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting: {experiment_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get data loaders\n",
    "    train_loader, val_loader = get_dataloaders(\n",
    "        batch_size=batch_size,\n",
    "        use_augmentation=use_augmentation\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = get_model(\n",
    "        model_type=model_type,\n",
    "        use_batchnorm=use_batchnorm,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    num_params = count_parameters(model)\n",
    "    print(f\"Model: {model_type.upper()}, Parameters: {num_params:,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=experiment_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"experiment_name\", experiment_name)\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"description\", description)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"dropout_rate\", dropout_rate)\n",
    "        mlflow.log_param(\"use_batchnorm\", use_batchnorm)\n",
    "        mlflow.log_param(\"use_augmentation\", use_augmentation)\n",
    "        mlflow.log_param(\"num_parameters\", num_params)\n",
    "        mlflow.log_param(\"device\", str(device))\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_acc = 0.0\n",
    "        best_model_state = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, criterion, optimizer, device\n",
    "            )\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "            \n",
    "            # Log metrics to MLflow\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
    "            \n",
    "            # Track best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                if save_model:\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Log final metrics\n",
    "        final_train_acc = train_acc\n",
    "        final_val_acc = val_acc\n",
    "        overfit_gap = final_train_acc - final_val_acc\n",
    "        \n",
    "        mlflow.log_metric(\"final_train_accuracy\", final_train_acc)\n",
    "        mlflow.log_metric(\"final_val_accuracy\", final_val_acc)\n",
    "        mlflow.log_metric(\"final_train_loss\", train_loss)\n",
    "        mlflow.log_metric(\"final_val_loss\", val_loss)\n",
    "        mlflow.log_metric(\"overfit_gap\", overfit_gap)\n",
    "        mlflow.log_metric(\"best_val_accuracy\", best_val_acc)\n",
    "        \n",
    "        # Get run ID\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        \n",
    "        # Prepare results\n",
    "        results = {\n",
    "            \"run_id\": run_id,\n",
    "            \"experiment_name\": experiment_name,\n",
    "            \"model_type\": model_type,\n",
    "            \"final_train_accuracy\": final_train_acc,\n",
    "            \"final_val_accuracy\": final_val_acc,\n",
    "            \"final_train_loss\": train_loss,\n",
    "            \"final_val_loss\": val_loss,\n",
    "            \"best_val_accuracy\": best_val_acc,\n",
    "            \"overfit_gap\": overfit_gap,\n",
    "            \"hyperparameters\": {\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"epochs\": epochs,\n",
    "                \"dropout_rate\": dropout_rate,\n",
    "                \"use_batchnorm\": use_batchnorm,\n",
    "                \"use_augmentation\": use_augmentation\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nFinal Train Accuracy: {final_train_acc:.4f}\")\n",
    "        print(f\"Final Val Accuracy: {final_val_acc:.4f}\")\n",
    "        print(f\"Best Val Accuracy: {best_val_acc:.4f}\")\n",
    "        print(f\"Overfit Gap: {overfit_gap:.4f}\")\n",
    "        \n",
    "        return results, best_model_state\n",
    "\n",
    "print(\"Main training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run All 5 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment name in MLflow\n",
    "mlflow.set_experiment(DAGSHUB_REPO_NAME)\n",
    "\n",
    "all_results = []\n",
    "best_result = None\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Baseline CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1, model_state1 = train_model(\n",
    "    experiment_name=\"exp1_baseline_cnn\",\n",
    "    model_type=\"cnn\",\n",
    "    use_batchnorm=False,\n",
    "    dropout_rate=0.0,\n",
    "    use_augmentation=False,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    description=\"Baseline CNN without regularization - observe overfitting\",\n",
    "    save_model=True\n",
    ")\n",
    "all_results.append(results1)\n",
    "if results1[\"best_val_accuracy\"] > best_val_acc:\n",
    "    best_val_acc = results1[\"best_val_accuracy\"]\n",
    "    best_result = results1\n",
    "    best_model_state = model_state1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: CNN + Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2, model_state2 = train_model(\n",
    "    experiment_name=\"exp2_cnn_regularization\",\n",
    "    model_type=\"cnn\",\n",
    "    use_batchnorm=True,\n",
    "    dropout_rate=0.5,\n",
    "    use_augmentation=False,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    description=\"CNN with BatchNorm and Dropout(0.5) - reduce overfitting\",\n",
    "    save_model=True\n",
    ")\n",
    "all_results.append(results2)\n",
    "if results2[\"best_val_accuracy\"] > best_val_acc:\n",
    "    best_val_acc = results2[\"best_val_accuracy\"]\n",
    "    best_result = results2\n",
    "    best_model_state = model_state2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: CNN + Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3, model_state3 = train_model(\n",
    "    experiment_name=\"exp3_cnn_augmentation\",\n",
    "    model_type=\"cnn\",\n",
    "    use_batchnorm=True,\n",
    "    dropout_rate=0.5,\n",
    "    use_augmentation=True,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    description=\"CNN with regularization and data augmentation - best generalization\",\n",
    "    save_model=True\n",
    ")\n",
    "all_results.append(results3)\n",
    "if results3[\"best_val_accuracy\"] > best_val_acc:\n",
    "    best_val_acc = results3[\"best_val_accuracy\"]\n",
    "    best_result = results3\n",
    "    best_model_state = model_state3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results4, model_state4 = train_model(\n",
    "    experiment_name=\"exp4_hyperparameter_tuning\",\n",
    "    model_type=\"cnn\",\n",
    "    use_batchnorm=True,\n",
    "    dropout_rate=0.5,\n",
    "    use_augmentation=True,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    epochs=15,  # Increased epochs\n",
    "    description=\"Hyperparameter tuning - optimized settings\",\n",
    "    save_model=True\n",
    ")\n",
    "all_results.append(results4)\n",
    "if results4[\"best_val_accuracy\"] > best_val_acc:\n",
    "    best_val_acc = results4[\"best_val_accuracy\"]\n",
    "    best_result = results4\n",
    "    best_model_state = model_state4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: Simple MLP (Underfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results5, _ = train_model(\n",
    "    experiment_name=\"exp5_mlp_comparison\",\n",
    "    model_type=\"mlp\",\n",
    "    use_batchnorm=False,\n",
    "    dropout_rate=0.0,\n",
    "    use_augmentation=False,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    description=\"Simple MLP - demonstrates underfitting on image data\",\n",
    "    save_model=False  # Don't save MLP\n",
    ")\n",
    "all_results.append(results5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create summary dataframe\n",
    "data = []\n",
    "for result in all_results:\n",
    "    row = {\n",
    "        \"Experiment\": result[\"experiment_name\"],\n",
    "        \"Model\": result[\"model_type\"].upper(),\n",
    "        \"Train Acc\": f\"{result['final_train_accuracy']:.4f}\",\n",
    "        \"Val Acc\": f\"{result['final_val_accuracy']:.4f}\",\n",
    "        \"Best Val Acc\": f\"{result['best_val_accuracy']:.4f}\",\n",
    "        \"Overfit Gap\": f\"{result['overfit_gap']:.4f}\"\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest Model: {best_result['experiment_name']}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save best model weights\n",
    "model_path = \"models/best_model.pt\"\n",
    "torch.save(best_model_state, model_path)\n",
    "print(f\"Best model saved to {model_path}\")\n",
    "\n",
    "# Save model info\n",
    "model_info_path = \"models/best_model_info.json\"\n",
    "with open(model_info_path, \"w\") as f:\n",
    "    json.dump(best_result, f, indent=2)\n",
    "print(f\"Model info saved to {model_info_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiments summary to CSV\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "summary_data = []\n",
    "for result in all_results:\n",
    "    row = {\n",
    "        \"Experiment\": result[\"experiment_name\"],\n",
    "        \"Model Type\": result[\"model_type\"],\n",
    "        \"Train Accuracy\": result[\"final_train_accuracy\"],\n",
    "        \"Val Accuracy\": result[\"final_val_accuracy\"],\n",
    "        \"Best Val Accuracy\": result[\"best_val_accuracy\"],\n",
    "        \"Overfit Gap\": result[\"overfit_gap\"],\n",
    "        \"Learning Rate\": result[\"hyperparameters\"][\"learning_rate\"],\n",
    "        \"Batch Size\": result[\"hyperparameters\"][\"batch_size\"],\n",
    "        \"Epochs\": result[\"hyperparameters\"][\"epochs\"],\n",
    "        \"Dropout\": result[\"hyperparameters\"][\"dropout_rate\"],\n",
    "        \"BatchNorm\": result[\"hyperparameters\"][\"use_batchnorm\"],\n",
    "        \"Augmentation\": result[\"hyperparameters\"][\"use_augmentation\"],\n",
    "        \"Run ID\": result[\"run_id\"]\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(\"results/experiments_summary.csv\", index=False)\n",
    "print(\"Results saved to results/experiments_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Files\n",
    "\n",
    "Run this cell to download the trained model and results to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download model files\n",
    "print(\"Downloading best_model.pt...\")\n",
    "files.download(\"models/best_model.pt\")\n",
    "\n",
    "print(\"Downloading best_model_info.json...\")\n",
    "files.download(\"models/best_model_info.json\")\n",
    "\n",
    "print(\"Downloading experiments_summary.csv...\")\n",
    "files.download(\"results/experiments_summary.csv\")\n",
    "\n",
    "print(\"\\nAll files downloaded!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Copy best_model.pt to your local MLOps/models/ folder\")\n",
    "print(\"2. Copy best_model_info.json to your local MLOps/models/ folder\")\n",
    "print(\"3. Copy experiments_summary.csv to your local MLOps/results/ folder\")\n",
    "print(\"4. Git add, commit, and push to GitHub\")\n",
    "print(\"5. GitHub Actions will build and push Docker image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. View Results in DagsHub\n",
    "\n",
    "Your experiment results are now available at:\n",
    "\n",
    "**https://dagshub.com/YOUR_USERNAME/MLOps** → Experiments tab → MLflow UI\n",
    "\n",
    "You can:\n",
    "- Compare all 5 experiments\n",
    "- View training curves (loss and accuracy over epochs)\n",
    "- See all hyperparameters and metrics\n",
    "- Share the link with your instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nView your results at:\")\n",
    "print(f\"https://dagshub.com/{DAGSHUB_USERNAME}/{DAGSHUB_REPO_NAME}\")\n",
    "print(f\"\\nMLflow UI:\")\n",
    "print(f\"{MLFLOW_TRACKING_URI}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
